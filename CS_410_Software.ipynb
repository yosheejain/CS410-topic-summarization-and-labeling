{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkX0H0h435lP"
      },
      "source": [
        "## Data Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NolgYzNd1kXV",
        "outputId": "fa746f78-6d03-41da-e1fb-c52c7e6631da"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import requests\n",
        "import re\n",
        "import pandas as pd\n",
        "import os\n",
        "import tqdm\n",
        "import json\n",
        "%pip install youtube_transcript_api\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from openai import OpenAI\n",
        "import csv\n",
        "import os.path\n",
        "import re\n",
        "import sys\n",
        "from io import StringIO\n",
        "import pandas as pd\n",
        "import argparse\n",
        "import ast\n",
        "\n",
        "api_key = \"Enter api key\"\n",
        "\n",
        "\n",
        "# Define the feature description to decode the features\n",
        "feature_description = {\n",
        "    \"id\": tf.io.FixedLenFeature([], tf.string),\n",
        "    \"labels\": tf.io.VarLenFeature(tf.int64),\n",
        "}\n",
        "\n",
        "# Function to parse a single example\n",
        "def _parse_function(example_proto):\n",
        "    return tf.io.parse_single_example(example_proto, feature_description)\n",
        "\n",
        "# Function to extract YouTube ID from the 4-character ID\n",
        "def get_youtube_id(four_char_id):\n",
        "    base_url = \"https://data.yt8m.org/2/j/i\"\n",
        "    sub_path = f\"/{four_char_id[:2]}/{four_char_id}.js\"\n",
        "    url = base_url + sub_path\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, verify=False)\n",
        "        response.raise_for_status()  # Raise an HTTPError for bad responses\n",
        "\n",
        "        # Extract the YouTube ID using a regular expression\n",
        "        match = re.search(r'i\\(\".*?\",\"(.*?)\"\\);', response.text)\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "        else:\n",
        "            return None\n",
        "    except requests.exceptions.RequestException:\n",
        "        return None\n",
        "\n",
        "# Function to get transcript for a YouTube video\n",
        "def get_transcript(video_id):\n",
        "    try:\n",
        "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "        return \" \".join([entry['text'] for entry in transcript])\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# Directory containing the TFRecord files\n",
        "tfrecord_dir = \"/content/files\"  # Update with your directory path\n",
        "output_dir = \"./output_json_files\"  # Directory to save JSON files\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process each TFRecord file\n",
        "for tfrecord_file in os.listdir(tfrecord_dir):\n",
        "    if tfrecord_file.endswith(\".tfrecord\"):\n",
        "        tfrecord_path = os.path.join(tfrecord_dir, tfrecord_file)\n",
        "        output_path = os.path.join(output_dir, f\"{os.path.splitext(tfrecord_file)[0]}.json\")\n",
        "        print(output_path)\n",
        "\n",
        "        # Create a dataset from the TFRecord file\n",
        "        dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
        "        dataset = dataset.map(_parse_function)\n",
        "\n",
        "        # Extract the 4-digit codes from the 'id' field and keep the labels and id together as a tuple\n",
        "        codes = [(example['id'].numpy().decode('utf-8'), example['labels'].values.numpy()) for example in dataset]\n",
        "\n",
        "        # Print the extracted codes\n",
        "        print(\"Extracted Codes:\", codes)\n",
        "\n",
        "\n",
        "        # Example usage\n",
        "        youtube_ids = []\n",
        "        for four_char_id, labels in tqdm.tqdm(codes):\n",
        "            print(f\"Getting YouTube ID for {four_char_id}\")\n",
        "            youtube_id = get_youtube_id(four_char_id)\n",
        "            if youtube_id != \"Error\":\n",
        "                youtube_ids.append([youtube_id, labels])\n",
        "\n",
        "        with open('Vocabulary.csv', mode='r') as file:\n",
        "            reader = csv.reader(file)\n",
        "            vocabulary = {rows[0]: rows[3] for rows in reader}\n",
        "\n",
        "        # Print the vocabulary\n",
        "        # print(\"Vocabulary:\", vocabulary)\n",
        "        # print(vocabulary['0'])\n",
        "\n",
        "        video_data = []\n",
        "        for youtube_id, labels in youtube_ids:\n",
        "            print(f\"  YouTube ID: {youtube_id}\")\n",
        "            for label in labels:\n",
        "                print(f\"  Label {label}: {vocabulary[str(label)]}\")\n",
        "            print(f\"  Labels: {[vocabulary[str(label)] for label in labels]}\")\n",
        "            video_data.append((youtube_id, [vocabulary[str(label)] for label in labels]))\n",
        "\n",
        "        data_to_write = []\n",
        "        for video_id, labels in video_data:\n",
        "            try:\n",
        "                transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "                transcript_text = \" \".join([entry['text'] for entry in transcript])\n",
        "                data_to_write.append({\n",
        "                    'video_id': video_id,\n",
        "                    'labels': labels,\n",
        "                    'transcript': transcript_text\n",
        "                })\n",
        "                print(\"Data for video ID\", video_id, \"added to JSON\")\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        # Write data to JSON file\n",
        "        with open(output_path, 'w') as json_file:\n",
        "            json.dump(data_to_write, json_file, indent=4)\n",
        "\n",
        "        print(f\"Finished processing {tfrecord_file}. Data saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWKvGuh6396Z"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwLll7Ys6H_d",
        "outputId": "c8466522-cc1e-4a6b-e677-547b1bbcdfd3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "# Initialize tools\n",
        "nltk.download('stopwords', quiet=True)\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Initialize the Porter Stemmer\n",
        "wstem = PorterStemmer()\n",
        "\n",
        "# Define punctuations and stopwords\n",
        "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Define the preprocess_text function\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocesses input text by:\n",
        "    - Removing HTML tags\n",
        "    - Converting to lowercase\n",
        "    - Removing punctuation\n",
        "    - Removing numbers\n",
        "    - Removing excess whitespace\n",
        "    - Tokenizing\n",
        "    - Removing stopwords\n",
        "    \"\"\"\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = ''.join(char for char in text if char not in punctuations)\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Tokenize (split into words)\n",
        "    words = text.split()\n",
        "\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Apply stemming\n",
        "    words = [wstem.stem(word) for word in words]\n",
        "\n",
        "    # Return cleaned text as a single string\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Folder containing all JSON files\n",
        "all_files_path = \"/content/output_json_files\"\n",
        "\n",
        "# Initialize an empty list to hold all data\n",
        "video_data = []\n",
        "\n",
        "# Read all JSON files from the directory\n",
        "for file_name in os.listdir(all_files_path):\n",
        "    if file_name.endswith(\".json\"):  # Only process JSON files\n",
        "        file_path = os.path.join(all_files_path, file_name)\n",
        "        with open(file_path, 'r') as file:\n",
        "            data = json.load(file)\n",
        "            video_data.extend(data)  # Assuming each file is a list of video records\n",
        "\n",
        "# Preprocess transcripts and collect data\n",
        "preprocessed_data = []\n",
        "corpus = []\n",
        "\n",
        "for item in video_data:\n",
        "    preprocessed_transcript = preprocess_text(item['transcript'])\n",
        "    corpus.append(preprocessed_transcript)\n",
        "    preprocessed_data.append({\n",
        "        \"video_id\": item[\"video_id\"],\n",
        "        \"cleaned_transcript\": preprocessed_transcript,\n",
        "        \"labels\": item[\"labels\"],\n",
        "        \"generated_labels\": None  # Placeholder for future labels\n",
        "    })\n",
        "\n",
        "# Apply TF-IDF to remove common words across all transcripts\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Remove words with low TF-IDF scores\n",
        "filtered_data = []\n",
        "for i, item in enumerate(preprocessed_data):\n",
        "    tfidf_scores = zip(vectorizer.get_feature_names_out(), X[i].toarray()[0])\n",
        "    significant_words = [word for word, score in tfidf_scores if score > 0.1]\n",
        "    filtered_text = ' '.join(significant_words)\n",
        "    filtered_data.append({\n",
        "        \"video_id\": item[\"video_id\"],\n",
        "        \"cleaned_transcript\": filtered_text,\n",
        "        \"labels\": item[\"labels\"],\n",
        "        \"generated_labels\": item[\"generated_labels\"]\n",
        "    })\n",
        "\n",
        "# Convert to DataFrame for structured representation\n",
        "df = pd.DataFrame(filtered_data)\n",
        "\n",
        "# Save the processed DataFrame to a CSV file\n",
        "output_path = \"cleaned_dataset.csv\"\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "# Print a sample to verify\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L29GQwSs6QIs"
      },
      "source": [
        "## Unigram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcMnBiT96QWS"
      },
      "outputs": [],
      "source": [
        "def split_into_segments(text):\n",
        "    if pd.isna(text):\n",
        "        return [\"\", \"\", \"\", \"\"]\n",
        "\n",
        "    words = text.split()\n",
        "    total_words = len(words)\n",
        "\n",
        "    if total_words == 0:\n",
        "        return [\"\", \"\", \"\", \"\"]\n",
        "\n",
        "    segment_size = max(1, total_words // 4)\n",
        "\n",
        "    segments = []\n",
        "    for i in range(0, total_words, segment_size):\n",
        "        segment = words[i:i + segment_size]\n",
        "        segments.append(\" \".join(segment))\n",
        "\n",
        "    if len(segments) > 4:\n",
        "        # Join the remaining segments with the 4th segment\n",
        "        segments[3] = segments[3] + \" \" + \" \".join(segments[4:])\n",
        "        segments = segments[:4]\n",
        "    elif len(segments) < 4:\n",
        "        while len(segments) < 4:\n",
        "            segments.append(\"\")\n",
        "\n",
        "    return segments\n",
        "\n",
        "df = pd.read_csv(\"cleaned_dataset.csv\")\n",
        "df['unigrams'] = df['cleaned_transcript'].apply(split_into_segments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrZFBlLy6Rhm",
        "outputId": "a670786a-c2bd-42e6-b592-03a7c9da0a1d"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict, Counter\n",
        "import math\n",
        "\n",
        "\n",
        "def calculate_unigram_language_model(transcript):\n",
        "  # Calculate the unigram language model for a given transcript.\n",
        "  if pd.isna(transcript):\n",
        "    return {}\n",
        "\n",
        "  words = transcript.split()\n",
        "  total_words = len(words)\n",
        "  if total_words == 0:\n",
        "    return {}\n",
        "\n",
        "  word_frequencies = Counter(words)\n",
        "  return {word: freq / total_words for word, freq in word_frequencies.items()}\n",
        "\n",
        "\n",
        "def calculate_scores(full_lm, segment_lm):\n",
        "  # Calculate the score for each word based on Full-LM and Segment-LM.\n",
        "\n",
        "  scores = {}\n",
        "  for word in segment_lm:\n",
        "    probability_full_lm = full_lm.get(word, 0)\n",
        "    probability_segment_lm = segment_lm[word]\n",
        "    scores[word] = -probability_full_lm + probability_segment_lm\n",
        "  return scores\n",
        "\n",
        "\n",
        "def extract_top_n_words(scores, n=5):\n",
        "  # Extract the top N words based on scores.\n",
        "\n",
        "  return [word for word, _ in sorted(scores.items(), key=lambda item: item[1], reverse=True)[:n]]\n",
        "\n",
        "\n",
        "def process_video_segments(full_transcript, segment_transcripts, n=5):\n",
        "  # Process the video and its segments to extract top N representative words for each segment.\n",
        "\n",
        "  if pd.isna(full_transcript) or not isinstance(segment_transcripts, list):\n",
        "    return {}\n",
        "\n",
        "  full_lm = calculate_unigram_language_model(full_transcript)\n",
        "  segment_word_map = {}\n",
        "\n",
        "  for i, segment in enumerate(segment_transcripts):\n",
        "    if pd.isna(segment) or segment == \"\":\n",
        "      segment_word_map[f\"Segment-{i+1}\"] = []\n",
        "      continue\n",
        "\n",
        "    segment_lm = calculate_unigram_language_model(segment)\n",
        "    scores = calculate_scores(full_lm, segment_lm)\n",
        "    print(scores)\n",
        "    top_words = extract_top_n_words(scores, n)\n",
        "    segment_word_map[f\"Segment-{i+1}\"] = top_words\n",
        "\n",
        "  return segment_word_map\n",
        "\n",
        "df['unigram_word_map'] = None\n",
        "for index, row in df.iterrows():\n",
        "  full_transcript = row['cleaned_transcript']\n",
        "  segment_transcripts = row['unigrams']\n",
        "  print(segment_transcripts)\n",
        "  segment_word_map = process_video_segments(full_transcript, segment_transcripts)\n",
        "  df.at[index, 'unigram_word_map'] = segment_word_map\n",
        "\n",
        "\n",
        "df['segments'] = df['unigram_word_map'].apply(lambda x: [v for v in x.values()])\n",
        "df.to_csv('unigram_word_map.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76adjZD--ody"
      },
      "source": [
        "## GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQVex3-x-pPr"
      },
      "outputs": [],
      "source": [
        "def query_gpt(querygive, api_key):\n",
        "    client = OpenAI(api_key=api_key)\n",
        "    #NOTE: update API key to run this part\n",
        "\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": querygive}],\n",
        "        stream=False\n",
        "    )\n",
        "\n",
        "    return chat_completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_edcpre-59N"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"unigram_word_map.csv\")\n",
        "df['segments'] = df['segments'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "df['GPT_answer'] = None\n",
        "batch_size = 1000\n",
        "\n",
        "df_small = df\n",
        "\n",
        "for index, row in df_small.iterrows():\n",
        "    segments = row['segments']\n",
        "    if segments is None or pd.isna(segments).all() or len(segments) == 0:\n",
        "        df_small.at[index, 'GPT_answer'] = \"\"\n",
        "    else:\n",
        "        coherent_segments = []\n",
        "        for segment in segments:\n",
        "            print(segment)\n",
        "            post_text = \"\"\"I am giving you some words that represent a topic. Make a coherent topic out of those words. Do not add additional meaning or inferences.\n",
        "                Try to restrict to those words alone as much as possible. Keep the topic as short as possible. Return the output in the following format:\n",
        "                Topic: <topic>. Do not include any other text.\"\"\" + \" \".join(segment)\n",
        "            coherent_segments.append(query_gpt(post_text, api_key))\n",
        "        df_small.at[index, 'GPT_answer'] = coherent_segments\n",
        "\n",
        "df_small.to_csv(\"processed_batch_gpt_410.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIlbAHzz_QDg"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4wYnAKF-8Oz",
        "outputId": "fd882b47-14eb-4a62-e075-02f1008b4cca"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ast  # To safely evaluate string representations of lists\n",
        "\n",
        "# Load the CSV file\n",
        "df_results = pd.read_csv(\"/content/results.csv\")\n",
        "\n",
        "# Drop the column 'generated_labels'\n",
        "df_results = df_results.drop(columns=['generated_labels'], errors='ignore')\n",
        "\n",
        "# Rename columns\n",
        "df_results = df_results.rename(columns={\n",
        "    'GPT_answer': 'generated_labels'\n",
        "})\n",
        "\n",
        "import csv\n",
        "\n",
        "# Load the vocabulary from the CSV file into a dictionary\n",
        "with open('/content/Vocabulary.csv', mode='r') as file:\n",
        "    reader = csv.reader(file)\n",
        "    vocabulary = {rows[0]: rows[3] for rows in reader}\n",
        "\n",
        "# Function to map vocabulary to labels\n",
        "def map_labels(labels, vocab):\n",
        "    if isinstance(labels, str):  # Convert string to list if necessary\n",
        "        labels = ast.literal_eval(labels)\n",
        "    return [vocab.get(str(label), \"Unknown\") for label in labels]\n",
        "\n",
        "\n",
        "# Apply the vocabulary mapping to the 'labels' column in the DataFrame\n",
        "df_results['annotated_labels'] = df_results['labels'].apply(lambda x: map_labels(x, vocabulary))\n",
        "\n",
        "# Display the updated DataFrame\n",
        "df_results.head()\n",
        "\n",
        "\n",
        "# # Function to parse topics from a single string\n",
        "def parse_topics(row):\n",
        "    # Check for null or empty values\n",
        "    if pd.isna(row) or not row.strip():\n",
        "        return []  # Return an empty list if the row is invalid\n",
        "\n",
        "    try:\n",
        "        # Remove brackets, split by \", \", and extract the portion after \"Topic: \"\n",
        "        topics = [item.split(\"Topic: \")[1].strip() for item in row.strip('[]').split(', ')]\n",
        "        return topics\n",
        "    except IndexError:\n",
        "        return []  # Return an empty list if the format is unexpected\n",
        "\n",
        "\n",
        "# Apply the function to the DataFrame column\n",
        "df_results['parsed_topics'] = df_results['generated_labels'].apply(parse_topics)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "df_results.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JEO59QQ0EFck",
        "outputId": "49f6eece-7526-49cc-df9a-cb69b97e4a62"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_curve\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "!pip install rouge_score\n",
        "from rouge_score import rouge_scorer\n",
        "!pip install Levenshtein\n",
        "from Levenshtein import ratio as levenshtein_ratio\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "rouge_scorer_instance = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "smoother = SmoothingFunction()\n",
        "\n",
        "\n",
        "THRESHOLD = 0.4\n",
        "\n",
        "\n",
        "\n",
        "df = df_results\n",
        "\n",
        "\n",
        "levenshtein_scores, bleu_scores, rouge_scores = [], [], []\n",
        "levenshtein_bool, bleu_bool, rouge_bool = [], [], []\n",
        "\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    ann = str(row[\"annotated_labels\"])\n",
        "    gen = str(row[\"parsed_topics\"])\n",
        "\n",
        "\n",
        "    lev_score = levenshtein_ratio(gen, ann)\n",
        "    levenshtein_scores.append(lev_score)\n",
        "    levenshtein_bool.append(lev_score > THRESHOLD)\n",
        "\n",
        "\n",
        "    reference = nltk.word_tokenize(ann.lower())\n",
        "    candidate = nltk.word_tokenize(gen.lower())\n",
        "    bleu_score = sentence_bleu([reference], candidate, smoothing_function=smoother.method1)\n",
        "    bleu_scores.append(bleu_score)\n",
        "    bleu_bool.append(bleu_score > THRESHOLD)\n",
        "\n",
        "\n",
        "    rouge_score = rouge_scorer_instance.score(ann, gen)['rougeL'].fmeasure\n",
        "    rouge_scores.append(rouge_score)\n",
        "    rouge_bool.append(rouge_score > THRESHOLD)\n",
        "\n",
        "\n",
        "similarity_df = pd.DataFrame({\n",
        "    \"Levenshtein\": levenshtein_scores,\n",
        "    \"BLEU\": bleu_scores,\n",
        "    \"ROUGE\": rouge_scores,\n",
        "    \"Levenshtein_Bool\": levenshtein_bool,\n",
        "    \"BLEU_Bool\": bleu_bool,\n",
        "    \"ROUGE_Bool\": rouge_bool\n",
        "})\n",
        "\n",
        "\n",
        "mean_scores = {\n",
        "    \"Levenshtein\": similarity_df[\"Levenshtein\"].mean(),\n",
        "    \"BLEU\": similarity_df[\"BLEU\"].mean(),\n",
        "    \"ROUGE\": similarity_df[\"ROUGE\"].mean()\n",
        "}\n",
        "\n",
        "\n",
        "f1_scores = {\n",
        "    \"Levenshtein\": f1_score([1] * len(levenshtein_bool), levenshtein_bool, zero_division=0),\n",
        "    \"BLEU\": f1_score([1] * len(bleu_bool), bleu_bool, zero_division=0),\n",
        "    \"ROUGE\": f1_score([1] * len(rouge_bool), rouge_bool, zero_division=0)\n",
        "}\n",
        "\n",
        "precision_scores = {\n",
        "    \"Levenshtein\": precision_score([1] * len(levenshtein_bool), levenshtein_bool, zero_division=0),\n",
        "    \"BLEU\": precision_score([1] * len(bleu_bool), bleu_bool, zero_division=0),\n",
        "    \"ROUGE\": precision_score([1] * len(rouge_bool), rouge_bool, zero_division=0)\n",
        "}\n",
        "\n",
        "recall_scores = {\n",
        "    \"Levenshtein\": recall_score([1] * len(levenshtein_bool), levenshtein_bool, zero_division=0),\n",
        "    \"BLEU\": recall_score([1] * len(bleu_bool), bleu_bool, zero_division=0),\n",
        "    \"ROUGE\": recall_score([1] * len(rouge_bool), rouge_bool, zero_division=0)\n",
        "}\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.bar(mean_scores.keys(), mean_scores.values())\n",
        "plt.title(\"Mean Similarity Scores\", fontsize=14, pad=20)\n",
        "plt.ylabel(\"Score\", fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "             f'{height:.2f}',\n",
        "             ha='center', va='bottom')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "metrics_df = pd.DataFrame({\n",
        "    \"Levenshtein\": [f1_scores[\"Levenshtein\"], precision_scores[\"Levenshtein\"], recall_scores[\"Levenshtein\"]],\n",
        "    \"BLEU\": [f1_scores[\"BLEU\"], precision_scores[\"BLEU\"], recall_scores[\"BLEU\"]],\n",
        "    \"ROUGE\": [f1_scores[\"ROUGE\"], precision_scores[\"ROUGE\"], recall_scores[\"ROUGE\"]]\n",
        "}, index=[\"F1-Score\", \"Precision\", \"Recall\"])\n",
        "\n",
        "\n",
        "ax = metrics_df.plot(kind=\"bar\", figsize=(12, 7))\n",
        "plt.title(\"Model Performance Metrics\", fontsize=14, pad=20)\n",
        "plt.ylabel(\"Score\", fontsize=12)\n",
        "plt.xlabel(\"Metrics\", fontsize=12)\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend(title=\"Metric\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "\n",
        "for container in ax.containers:\n",
        "    ax.bar_label(container, fmt='%.2f', padding=3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
