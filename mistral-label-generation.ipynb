{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install accelerate\n",
    "!pip install sentencepiece\n",
    "import sentencepiece\n",
    "import torch\n",
    "import accelerate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import gc\n",
    "\n",
    "NUM_GEN = 0\n",
    "\n",
    "def load_model(model_name=\"mistralai/Mistral-7B-Instruct-v0.2\", only_tokenizer=False, quant_type=None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "    if not only_tokenizer:\n",
    "        if quant_type is not None:\n",
    "            if quant_type == '8_bit':\n",
    "                print(\"loading 8 bit model\")\n",
    "                model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', torch_dtype=torch.float16, load_in_8bit=True)\n",
    "            elif quant_type == '4_bit':\n",
    "                print(\"loading 4 bit model\")\n",
    "                model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', bnb_4bit_quant_type=\"nf4\", load_in_4bit=True,  bnb_4bit_compute_dtype=torch.float16)\n",
    "        else:\n",
    "            print('no quantization, loading in fp16')\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', torch_dtype=torch.float16)\n",
    "        #check device of all model tensors\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'cuda' not in str(param.device):\n",
    "                print(f\"param {name} not on cuda\")\n",
    "        return tokenizer, model\n",
    "    else:\n",
    "        return tokenizer, None\n",
    "\n",
    "def query_model(prompts, qa_model, tokenizer, do_sample=True, top_k=10,\n",
    "                num_return_sequences=1, max_length=1024, temperature=1.0, INPUT_DEVICE='cuda:0'):\n",
    "    global NUM_GEN\n",
    "    # preprocess prompts:\n",
    "    import time\n",
    "    assert len(prompts) == 1\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": f\"{prompts[0]}\"},]\n",
    "\n",
    "    encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "    model_inputs = encodeds.to(INPUT_DEVICE)\n",
    "    start_time = time.time()\n",
    "    generated_ids = qa_model.generate(model_inputs, max_new_tokens=max_length, do_sample=do_sample, temperature=temperature)\n",
    "    print(f\"Time taken for model: {time.time() - start_time}\")\n",
    "    generated_ids = generated_ids[:, model_inputs.shape[-1]:]\n",
    "    decoded = tokenizer.batch_decode(generated_ids.detach().cpu(), skip_special_tokens=True)\n",
    "\n",
    "    NUM_GEN += 1\n",
    "    if NUM_GEN % 50 == 0:\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    model_ans = decoded[0].strip()\n",
    "    del model_inputs, generated_ids\n",
    "    return [model_ans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, qa_model = load_model(\"mistralai/Mistral-7B-Instruct-v0.2\", only_tokenizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# read from CSV, 5 words per label\n",
    "df = pd.read_csv(\"/Users/yoshe/Desktop/CS410-topic-summarization-and-labeling/video_transcripts_with_corrected_list_of_lists.csv\")\n",
    "df['Generated_labels'] = ''\n",
    "batch_size = 1000\n",
    "\n",
    "# Calculate the number of batches\n",
    "num_batches = math.ceil(len(df) / batch_size)\n",
    "\n",
    "for batch in range(num_batches):\n",
    "    start_idx = batch * batch_size\n",
    "    end_idx = min((batch + 1) * batch_size, len(df))\n",
    "    \n",
    "    # Process each batch\n",
    "    for index in range(start_idx, end_idx):\n",
    "        print(index)\n",
    "        row = df.iloc[index]\n",
    "        list_of_labels = list(row['Labels'])\n",
    "        list_of_generated_labels = []\n",
    "        for label in list_of_labels:\n",
    "            string_of_words = \"\"\n",
    "            for word in label:\n",
    "                string_of_words += word + \" \"\n",
    "            post_text = \"You are being given a list of words, and you need to generate a coherent label for the video based on the words. The label should should not be very long, so keep it short. Just return the label and no other text.The words are: \" + string_of_words\n",
    "            model_answers = query_model([post_text], qa_model, tokenizer, temperature=0.000001, INPUT_DEVICE=\"cuda\", do_sample=False)\n",
    "            list_of_generated_labels.append(model_answer[0])\n",
    "        df.at[index, 'Generated_labels'] = list_of_generated_labels\n",
    "    \n",
    "    # Save the dataframe after each batch\n",
    "    df.to_csv(f\"processed_batch_mistral_{batch}.csv\", index=False)\n",
    "    print(f\"Batch {batch + 1}/{num_batches} processed and saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
