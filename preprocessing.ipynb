import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
import string

# Download necessary NLTK data files
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

def preprocess_text(text):
    # Tokenize text
    tokens = word_tokenize(text.lower())
    
    # Remove stop words and punctuation
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]
    
    # Apply stemming and lemmatization
    stemmer = PorterStemmer()
    lemmatizer = WordNetLemmatizer()
    tokens = [stemmer.stem(lemmatizer.lemmatize(word)) for word in tokens]
    
    return tokens



def compute_tfidf(transcripts):
    """
    Compute Term Frequency-Inverse Document Frequency (TF-IDF) for transcripts to remove common words.
    Returns a dictionary of terms and their scores.
    This step weights terms by their importance within a segment of a transcript. 
    Common words across the entire video are downweighted, ensuring that unique, topic-specific words are emphasized.
    """
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(transcripts)  # TF-IDF matrix
    feature_names = vectorizer.get_feature_names_out()
    
    # Map terms to their scores for each transcript
    tfidf_scores = []
    for row in tfidf_matrix:
        tfidf_scores.append(dict(zip(feature_names, row.toarray().flatten())))
    
    return tfidf_scores



# Example data
(* data = {
    "video_id": ["vid1", "vid2"],
    "raw_transcript": [
        "This is the first transcript for video one.",
        "This is another example transcript for video two."
    ],
    "annotated_labels": [["label1", "label2"], ["label3", "label4"]]
} *)

# Preprocess transcripts
data["cleaned_transcript"] = [preprocess_text(transcript) for transcript in data["raw_transcript"]]

# Compute TF-IDF for cleaned transcripts
tfidf_scores = compute_tfidf([" ".join(tokens) for tokens in data["cleaned_transcript"]])

# Organize into a DataFrame
df = pd.DataFrame({
    "video_id": data["video_id"],
    "cleaned_transcript": [" ".join(tokens) for tokens in data["cleaned_transcript"]],
    "annotated_labels": data["annotated_labels"],
    "tfidf_scores": tfidf_scores,
    "generated_labels": [None] * len(data["video_id"])  # Placeholder for generated labels
})

# Save the table for further use
df.to_csv('preprocessed_data.csv', index=False)
print("Data saved successfully!")
