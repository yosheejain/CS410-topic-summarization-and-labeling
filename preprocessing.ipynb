{
    "name": "Video Summarization Preprocessing",
    "description": "Python script for preprocessing video transcripts by cleaning text, applying stemming and lemmatization, and calculating TF-IDF scores for topic-specific word emphasis.",
    "main_script": "preprocess_transcripts.py",
    "language": "Python",
    "dependencies": {
        "libraries": [
            "nltk",
            "scikit-learn",
            "pandas"
        ]
    },
    "script": {
        "code": [
            "import nltk",
            "from nltk.corpus import stopwords",
            "from nltk.tokenize import word_tokenize",
            "from nltk.stem import PorterStemmer, WordNetLemmatizer",
            "from sklearn.feature_extraction.text import TfidfVectorizer",
            "import pandas as pd",
            "import string",
            "",
            "# Download necessary NLTK data files",
            "nltk.download('punkt')",
            "nltk.download('stopwords')",
            "nltk.download('wordnet')",
            "",
            "def preprocess_text(text):",
            "    \"\"\"",
            "    Preprocess text by removing stop words, punctuation, and applying stemming and lemmatization.",
            "    \"\"\"",
            "    tokens = word_tokenize(text.lower())",
            "    stop_words = set(stopwords.words('english'))",
            "    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]",
            "    stemmer = PorterStemmer()",
            "    lemmatizer = WordNetLemmatizer()",
            "    tokens = [stemmer.stem(lemmatizer.lemmatize(word)) for word in tokens]",
            "    return tokens",
            "",
            "def compute_tfidf(transcripts):",
            "    \"\"\"",
            "    Compute TF-IDF to remove common words and highlight topic-specific terms.",
            "    \"\"\"",
            "    vectorizer = TfidfVectorizer()",
            "    tfidf_matrix = vectorizer.fit_transform(transcripts)",
            "    feature_names = vectorizer.get_feature_names_out()",
            "    tfidf_scores = []",
            "    for row in tfidf_matrix:",
            "        tfidf_scores.append(dict(zip(feature_names, row.toarray().flatten())))",
            "    return tfidf_scores",
            "",
            "# Example data",
            "data = {",
            "    \"video_id\": [\"vid1\", \"vid2\"],",
            "    \"raw_transcript\": [",
            "        \"This is the first transcript for video one.\",",
            "        \"This is another example transcript for video two.\"",
            "    ],",
            "    \"annotated_labels\": [[\"label1\", \"label2\"], [\"label3\", \"label4\"]]",
            "}",
            "",
            "# Preprocess transcripts",
            "data[\"cleaned_transcript\"] = [preprocess_text(transcript) for transcript in data[\"raw_transcript\"]]",
            "",
            "# Compute TF-IDF for cleaned transcripts",
            "tfidf_scores = compute_tfidf([\" \".join(tokens) for tokens in data[\"cleaned_transcript\"]])",
            "",
            "# Organize into a DataFrame",
            "df = pd.DataFrame({",
            "    \"video_id\": data[\"video_id\"],",
            "    \"cleaned_transcript\": [\" \".join(tokens) for tokens in data[\"cleaned_transcript\"]],",
            "    \"annotated_labels\": data[\"annotated_labels\"],",
            "    \"tfidf_scores\": tfidf_scores,",
            "    \"generated_labels\": [None] * len(data[\"video_id\"])",
            "})",
            "",
            "# Save the table for further use",
            "df.to_csv('preprocessed_data.csv', index=False)",
            "print(\"Data saved successfully!\")"
        ]
    },
    "usage": [
        "1. Install dependencies using `pip install nltk scikit-learn pandas`.",
        "2. Run the script: `python preprocess_transcripts.py`.",
        "3. Output CSV: `preprocessed_data.csv` contains the processed data."
    ]
}
